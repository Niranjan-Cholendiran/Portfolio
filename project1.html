<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Niranjan's Portfolio</title>
    <link rel="stylesheet" href="style_project_1.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/fontawesome.min.css" integrity="sha384-BY+fdrpOd3gfeRvTSMT+VUZmA728cfF9Z2G42xpaRkUGu2i3DyzpTURDo5A6CaLK" crossorigin="anonymous">

    <script src="https://kit.fontawesome.com/90171aec82.js" crossorigin="anonymous"></script>
</head>
<body>
<div id="header">
    <div class="container">
        <nav>
            <a href="index.html#header"><img src="images/logo.png" class="logo"></a>
            <ul class="menu" id="sidemenu">
                <li><a href="index.html#header">Home</a></li>
                <li><a href="index.html#about">About Me</a></li>
                <li><a href="index.html#projects">My Work</a>
                    <div class="sub-menu-1"> 
                        <ul>
                            <li class="sub-menu-2-hover"><a href="project1.html#header">Inventory Optimization (CSCI 5622)</a>
                                <div class="sub-menu-2">
                                    <ul>
                                        <li><a href="#Introduction-Content" onclick="opentab('Introduction-Content')">Introduction</a></li>
                                        <li><a href="#Data-Preparation-Content" onclick="opentab('Data-Preparation-Content')">DataPrep_EDA</a></li>
                                        <li class="sub-menu-3-hover"><a href="#Clustering-Content" onclick="opentab('Clustering-Content')">Modeling</a>
                                            <div class="sub-menu-3">
                                                <ul>
                                                    <li><a href="#Clustering-Content" onclick="opentab('Clustering-Content')">Clustering</a></li>
                                                    <li><a href="#ARM-Content" onclick="opentab('ARM-Content')">ARM</a></li>
                                                    <li><a href="#NB-Content" onclick="opentab('NB-Content')">Naïve Bayes</a></li>
                                                    <li><a href="#DT-Content" onclick="opentab('DT-Content')">Decision Tree</a></li>
                                                    <li><a href="#SVM-Content" onclick="opentab('SVM-Content')">SVMs</a></li>
                                                    <li><a href="#Conclusion-Content" onclick="opentab('Conclusion-Content')">Conclusion</a></li>
                                                </ul>
                                            </div>                                        
                                        </li>
                                    </ul>
                                </div>
                            </li>


                            <li class="sub-menu-2-hover"><a href="project2.html#header">B2C Brand Perception (INFO 5871)</a>
                                <div class="sub-menu-2">
                                    <ul>
                                        <li><a href="project2.html#navigation-content">Introduction</a></li>
                                        <li><a href="project2.html#navigation-content">Data Prep</a></li>
                                        <li><a href="project2.html#navigation-content">Data Exploration</a></li>
                                        <li class="sub-menu-3-hover"><a href="#">Modeling</a>
                                            <div class="sub-menu-3">
                                                <ul>
                                                    <li><a href="project2.html#navigation-content">Clustering</a></li>
                                                    <li><a href="project2.html#navigation-content">LDA</a></li>
                                                    <li><a href="project2.html#navigation-content">ARM</a></li>
                                                    <li><a href="project2.html#navigation-content">Naïve Bayes</a></li>
                                                    <li><a href="project2.html#navigation-content">Decision Tree</a></li>
                                                    <li><a href="project2.html#navigation-content">SVM</a></li>
                                                    <li><a href="project2.html#navigation-content">Neural Netowrk</a></li>
                                                    <li><a href="project2.html#navigation-content">Conclusion</a></li>
                                                </ul>
                                            </div>
                                        </li>
                                    </ul>
                                </div>
                            </li>

                            
                            <li class="sub-menu-2-hover"><a href="project3.html#header">Dock Watch (CSCI 5502)</a>
                                <div class="sub-menu-2">
                                    <ul>
                                        <li><a href="project3.html#Introduction-Content">Introduction</a></li>
                                        <li><a href="project3.html#navigation-content">Data Preparation</a></li>
                                        <li><a href="project3.html#navigation-content">Data Exploration</a></li>
                                        <li><a href="project3.html#navigation-content">Modeling</a></li>
                                    </ul>
                                </div>
                            </li>
                        </ul>
                    </div>                
                </li>
                <li><a href="index.html#contact">Contact</a></li>
                <i class="fas fa-times" onclick="closemenu()"></i>
            </ul>
            <i class="fas fa-bars" onclick="openmenu()"></i>
        </nav>
        <div class="header-text">
            <h1>Inventory Optimization</h1>
        </div>
        <div class="image-credit"><a href="https://www.freepik.com/free-ai-image/diminishing-perspective-old-archive-shelves-glow-generated-by-ai_41283910.htm#query=inventory&position=41&from_view=search&track=sph&uuid=b9640a9c-497c-4233-b6ad-bb5a98163567">Image By vecstock</a>
        </div>
    </div>
</div>

<!-- -----------Project Contents---------- -->

<div class="container" id="navigation-content">
    <div class="tab-titles" >
        <p class="tab-links active-link" onclick="opentab('Introduction-Content')">Introduction</p>
        <p class="tab-links" onclick="opentab('Data-Preparation-Content')">Data Prep</p>
        <p class="tab-links" onclick="opentab('Data-Exploration-Content')">Data Exploration (EDA)</p>
        <p class="tab-links" onclick="opentab('Clustering-Content')">Clustering</p>
        <p class="tab-links" onclick="opentab('ARM-Content')">ARM</p>
        <p class="tab-links" onclick="opentab('NB-Content')">Naive Bayes</p>
        <p class="tab-links" onclick="opentab('DT-Content')">Decision Tree</p>
        <p class="tab-links" onclick="opentab('SVM-Content')">SVM</p>
        <p class="tab-links" onclick="opentab('Conclusion-Content')">Conclusion</p>
    </div>

    <div class="tab-contents active-tab" id="Introduction-Content">
        <div class="intro">
            <h1 class="sub-title">Introduction</h1>
            <br><br>
            <p>Have you ever experienced the frustration of finding your favourite product out of stock just when you're ready to make a purchase? Well, it's not just a letdown for you but also a significant loss for the retailers. In 2018, retailers lost out on $300 million in revenue due to poor inventory management, and that was before the pandemic. Pre-pandemic, the odds of shoppers seeing an out-of-stock message were 1 in 200. That figure rose to 1 in 59 in 2022, a 235% increase (<a href="https://www.retailbrew.com/stories/2023/04/25/why-inventory-management-is-important-for-retailers" target="_blank">source</a>). In a survey, almost half of respondents (46%) indicated that overbuying inventory and buying the wrong type of products are top contributing factors to markdowns(<a href="https://www.mytotalretail.com/article/hidden-costs-of-poor-inventory-management/" target="_blank">source</a>).</p>
            <br>
            <img src="images/inventrory_intro_quote.jpg" class= "intro-quote">
            <br>
            <p>This results in dissatisfied customers, decreased profits, high holding costs, expired inventory, and inevitable write-offs. But, who bears the responsibility for this inventory mismanagement? Products don't magically appear on shelves, right? They go through a complex distribution process involving multiple middlemen. In a typical One Level Channel distribution, the retailer stands between the manufacturer and the customer, serving as the crucial link responsible for managing inventory to meet customer demand and maximize profit.</p>
            <br>
            <img src="images/inventrory_one_level_distribution.png" class= "intro-quote">
            <br>
            <p>Yet, managing inventory is far from simple. It presents countless challenges, including poor demand forecasting, the impact of seasonality and trends, delays in purchasing, limitations in production capacity, delayed fulfilment, and more. Therefore, every retailer must implement a <span>framework to optimize inventory management by identifying the ideal time for purchases</span> to minimize overstocking or stockouts while maximizing revenue. The framework must be capable of answering the following questions:</p>
            <br>
            <ol style="padding-left: 80px;">
                <li>What product categories are in the highest demand?</li>
                <li>Which factors influence demand?</li>
                <li>What quantities of fast, slow and non-moving products are available in the inventory?</li>
                <li>What quantities of high and low profitable products are available in the inventory?</li>
                <li>What is the store’s average inventory replacement rate (or Inventory turnover rate), and how does it vary across product categories?</li>
                <li>Which vendors are reliable and have shorter fulfilment times?</li>
                <li>What is the fulfilment capacity of vendors?</li>
                <li>What are the associated purchase costs for each order?</li>
                <li>How level of demand can be expected in the upcoming days?</li>
                <li>Are there any emerging trends or consumer preferences that may impact future demand patterns?</li>
            </ol>
            <br>
            <p>Very few companies have achieved excellence in optimizing their inventory. For instance, Companies like Amazon, Walmart, and Coca-Cola have successfully implemented predictive analytics in their inventory management systems, resulting in significant cost savings and improved operational efficiency. Particularly, Amazon uses predictive analytics to manage its vast inventory effectively. By analyzing customer buying behaviour, Amazon can predict product demand with a high degree of accuracy and adjust its inventory levels accordingly. This has helped Amazon reduce its inventory holding costs and improve customer satisfaction (<a href="https://www.brisklogic.co/the-predictive-analytics-in-inventory-management/" target="_blank">source</a>).</p>
            <br><p>In summary, effective inventory management is essential for retailers to remain competitive in today's dynamic marketplace. By embracing technological advancements and implementing robust inventory optimization strategies, retailers can minimize stock-related challenges, maximize profitability, and deliver superior value to customers.</p>
        </div>
    </div>

    <div class="tab-contents" id="Data-Preparation-Content">
        <div class="intro">
            <h1 class="sub-title">Data Collection</h1>
            <br><br>
            <p>This project requires a thorough level of data encompassing sales, product, inventory, and purchase details. Accordingly, the data was collected from 2 major sources:</p>
            <br><br>
            <h3>Source 1: PWC’s Inventory Analysis Case Study Data files</h3>
            <br><br>
            <p>PWC offers a combination of 6 datasets covering end-to-end product distribution operations of Bibitor LLC, a liquor store chain in the fictional state of Lincoln. It is a major retailer with approximately 80 locations and total sales more than $450 million within a single year. <b>These datasets cover all transactions occurring between the retailer (Bibitor), vendors and customers throughout the year 2016.</b></p>
            <br>
            <h4>Raw Data Overview:</h4>
            <ul>
                <li>1. Beginning & Ending Inventory <i>(BegInvFINAL12312016.csv & EndInvFINAL12312016.csv):</i>
                    <ul>
                        <li>Description: Provides information of the products’ inventory availability at the beginning and end of the year across all stores.</li>
                        <li>Data Granularity: InventoryId</li>
                    </ul>
                </li>
                <li>2.  Purchases <i>(PurchasesFINAL12312016.csv):</i>
                    <ul>
                        <li>Description: Provides the complete purchase history of products by the retailer from various vendors, including fulfilment details.</li>
                        <li>Data Granularity: Store + PONumber + Brand + ReceivingDate</li>
                    </ul>
                </li>
                <li>3.  Purchase Prices <i>(2017PurchasePricesDec.csv):</i>
                    <ul>
                        <li>Description: Contains the purchase costs incurred by the retailer for each product</li>
                        <li>Data Granularity: Brand (A combination of product SKU, vendor, and purchase price)</li>
                    </ul>
                </li>
                <li>4.	Vendor Invoices <i>(VendorInvoices12312016.csv):</i>
                    <ul>
                        <li>Description: Presents details of the purchase invoices for every purchase order made.</li>
                        <li>Data Granularity: POnumber</li>
                    </ul>
                </li>
                <li>5.	Sales (SalesFINAL12312016.csv):</i>
                    <ul>
                        <li>Description: Provides the sales history of products across all retail stores</li>
                        <li>Data Granularity: Store + Brand + SalesDate</li>
                    </ul>
                </li>
                <li>Download the raw data <a href="https://www.pwc.com/us/en/careers/university-relations/data-and-analytics-case-studies-files.html" target="_blank">here</a></li>
            </ul>
            <br>
            <h3>Source 2: OpenAI API</h3>
            <br><br>
            <p>Data obtained from Source 1 primarily centres on transactions but lacks essential product details. The Bibitor dataset comprises over 11,000 Liquor SKUs, yet only provides descriptions without further particulars. Understanding the drivers of product demand necessitates comprehensive product details. Therefore, the <a href="https://openai.com/blog/openai-api" target="_blank">OpenAI API</a> was used to gather the following information for each product:</p>
            <br>
            <ol>
                <li><b>Alcohol Type:</b> The category of alcohol, including options such as Wine, Vodka, Whiskey, Rum, Tequila, and more.</li>
                <li><b>Alcohol By Volume (ABV):</b> The percentage of alcohol content in the beverage, typically ranging from 0% to 100%.</li>
                <li><b>Flavor Profile:</b> The sensory characteristics of the drink, encompassing flavor such as Herbal, Sweet, Spicy, Floral, Sour, and others.</li>
            </ol>
            <br>
            <p>Download the raw data <a href="https://drive.google.com/drive/folders/1CQmhaoZkbAAt9IC7jwQBY4rPpPmHuxhT?usp=sharing" target="_blank">here</a></p>
            <br>
            <h4>API Components:</h4>
            <br>
            <P>Open API provides paid APIs to access their AI transformer models. For this project, the GPT model with the following components were used:</P>
            <ul>
                <li>1. Endpoint: <i>ChatCompletion</i></li>
                <li>2. Parameters:</li>
                <ul>
                    <li>Model: <i>“gpt-3.5-turbo”</i></li>
                    <li>Message: <i>Input Query</i></li>
                </ul>
            </ul>
            <img src="images/inventory_api_components.jpg" class= "intro-quote">
            <p>Download the full API code <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/Data%20Collection/02.%20OpenAI%20Product%20Info%20Data%20Collection.ipynb" target="_blank">here</a></p>

            <br><br>
            <h1 class="sub-title">Data Preparation</h1>
            <br><br>
            <p>The collected raw data possessed the required granularity. However, it lacked structured, with tables containing redundant and overlapping information, as well as a few extraneous columns and unclean records. <b>Hence, a new set of analytical base tables with a structured Entity Relationship has been designed.</b></p>
            <br>
                <div style="text-align: center;">
                    <iframe width="1000" height="500" src='https://dbdiagram.io/e/65bd8a35ac844320ae58ac4d/65c03f39ac844320ae6d38b2'> </iframe>
                    <p style="text-align:center;"><i style="font-size: 14px;">Note: Hover through the ERD diagram to highlight relationships and read description.</i></p>
                </div>
            <br>
            <p>These analytical tables are high-quality, clean datasets compiled from both sources and are ready for direct utilization in analytical tasks. Note that these datasets are non-aggregated tables that can be seamlessly combined as per requirements.</p>
            <br>
            <h4>Data Preparation Steps:</h4>
            <ul>
                <li>1. Standardized “Size” column:</i>
                    <ul>
                        <li>Initially, the size column contained disparate units and noisy data. To rectify this, it was transformed into a uniform millimeter unit, following the removal of irrelevant noise.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_1.jpg" class= "DataPrep-Step">
                </li>

                <li>2. Data extraction from OpenAI dataset:</i>
                    <ul>
                        <li>OpenAI responses were noisy with extraneous information. Hence, only the relevant information was extracted.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_2.jpg" class= "DataPrep-Step">
                </li>

                <li>3. Generated unique product ID:</i>
                    <ul>
                        <li>In the absence of a designated ID for product SKUs in the raw dataset, the entire product information had to be replicated across all tables containing product data. To streamline this, SKU details from all raw tables were combined, and distinct IDs were created for each product SKU.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_3.jpg" class= "DataPrep-Step">
                </li>

                <li>4. Formatted dates:</i>
                    <ul>
                        <li>All date entries were reformatted to the standard YYYY-MM-DD format.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_4.jpg" class= "DataPrep-Step">
                </li>

                <li>5. Handled missing values:</i>
                    <ul>
                        <li>The OpenAI data contains a few missing records that were replaced with "Others" in "FlavorProfile" and "AlcoholType" column, and with Mean in "ABV".</li>
                    </ul>
                    <img src="images/inventory_DataPrep_5.jpg" class= "DataPrep-Step">
                </li>

                <li>6. Fixed data types</i>
                    <ul>
                        <li>All numeric ID columns were converted to object types.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_6.jpg" class= "DataPrep-Step">
                </li>

                <li>7. Combined begin and ending inventory table</i>
                    <ul>
                        <li>Begining and endign inventory tables were merged to prevent redundancy.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_7.jpg" class= "DataPrep-Step">
                </li>

                <li>8. Constructed dedicated info tables</i>
                    <ul>
                        <li>Built dedicated tables for product, vendor, and store details, aiming to minimize redundancy within other tables and decrease overall data size.</li>
                    </ul>
                    <img src="images/inventory_DataPrep_8.jpg" class= "DataPrep-Step">
                </li>
                <p>Download the data preprataion code <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/Data%20Preparation/01.%20Analytical%20Base%20Tables%20Preparation.ipynb" target="_blank">here</a></p>
            </ul>
            <br>

        </div>
    </div>

    <div class="tab-contents" id="Data-Exploration-Content">
        <div class="intro">
            <h1 class="sub-title">Data Exploration</h1>
            <br><br>
            <p>Find insights from the data that are crucial to optimize the inventory below:</p>
            <br><br>
            <ul style="padding-left: 10px;"">

                <li><h4 style="color: #61b752; font-weight: bold;">1. Store, product, and vendors volume:</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_01.jpg" class= "DataPrep-Step">
                        <li>Sales and purchase data cover the entire year of 2016.</li>
                        <li>The dataset includes information from 80 stores, 132 vendors, 11K brands, and 11K products.</li>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">2. Sampling:</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_02.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>For this project, the sample is taken as Store 51, which ranks close to median in the:</li>
                        <ul>
                            <li>Quantity sold (Rank 40)</li>
                            <li>Total sales value (Rank 40)</li>
                            <li>Number of unique products sold (53)</li>
                            <li>Number of unique days sales occurred (Nearly every day)</li>
                        </ul>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">3. Store ID 51’s sales and purchase summary:</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_03.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>The store has purchased around 2K unique products from 73 vendors.</li>
                        <li>Purchases and sales exceeded 300K products.</li>
                        <li>Highest demand was observed in Q4 of 2016.</li>
                    </ul>
                </li>
                <br>


                <li><h4 style="color: #61b752; font-weight: bold;">4. Which category of liquors dominated the store’s inventory?</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_04.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>Fruity-flavored liquors and wine alcoholic types were prevalent in the inventory.</li>
                        <li>Gin had the highest average level of alcohol volume, while Vodka had the highest ABV variance.</li>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">5. Which category of liquors had a higher price per product and was more profitable?</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_05.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>Floral flavoured drinks were the most expensive (385% above overall average) and profitable (317% above overall average).</li>
                        <li>Whiskeys were the most expensive (55% above overall average) and profitable (33% above overall average) alcoholic type.</li>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">6. Which category of liquors had higher demand and generated greater profits?</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_06.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>Despite Floral flavour and Whiskeys having the highest margin per product, Fruity flavor and Wines generated more profit fuelled by their high demand.</li>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">7. Which category of liquors were purchased more from the vendors?</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_07.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>To meet demand, Fruity flavor and Wines were the top choices for purchase.</li>
                    </ul>
                </li>
                <br>

                <li><h4 style="color: #61b752; font-weight: bold;">8. How long does the vendors take to fulfil the order?</h4></i>
                    <br>
                    <ul>
                        <img src="images/EDA_08.jpg" class= "DataPrep-Step" style="max-width: 90%;">
                        <li>Vendors typically take around 8 days to deliver the products, with some vendors managing delivery times as low as 4 days.</li>
                    </ul>
                </li>
                <br>
                
                <li>The above visualizations were built using <a href="https://www.microsoft.com/en-us/power-platform/products/power-bi" target="_blank">Microsoft Power BI</a></li>
            </ul>
            
            <br>
        </div>
    </div>

    <div class="tab-contents" id="Clustering-Content">
        <div class="intro">
            <h1 class="sub-title">Clustering</h1>
            <br><br>
            <p>Clustering is as an unsupervised learning method in machine learning, dividing data based on feature distances. Its goal is to group similar records within clusters (cohesion) while maintaining dissimilarity from objects in other clusters (separation). There are multiple ways to implement clustering however the most popular implementation includes K-Means and Hierarchical Clustering.</p>
            <br>
            <p><b>K-Means Clustering: </b>This partitioning technique clusters data into a set number of clusters by aggregating the K closest points to a designated point. Various distance metrics (Euclidean, Manhattan, etc.) can be utilized based on the context.</b></p>
            <img src="images/clustering_kmeans.png" class= "DataPrep-Step" style="max-width: 65%;">
            <br>
            <p><b>Hierarchical Clustering: </b>This method organizes data hierarchically, constructing dendrogram tree structures. Agglomerative clustering starts with the assumption that each point forms its own cluster, gradually merging them until ideally only one cluster remains. Similar to K-Means, diverse distance metrics and linkage methods are available, chosen according to the specific case.</b></p>
            <img src="images/clustering_hier.png" class= "DataPrep-Step" style="max-width: 30%;">
            <br>
            <p>For this application, both clustering and hierarchical clustering are applied in this scenario to categorize liquor items based on demand and profitability. This will aid in stock planning to maximize profit.</b></p>
            <br>
            <h3>Data Preparation</h3>
            <br><br>
            <p>Liquor information, its sales history and net profit data is required to implement and infer the clustering results. Hence, data clustering data was prepared by:</b></p>
            <br>
            <ol>
                <li>Left joining Product_Info with Product_Purchase_Sale_Price tables and taking purchase and sales price from the right table to calculate profit_per_product.</li>
                <br>
                <li>Right joining the resultant data with SalesHistory table to calculate demand per day.</li>  
            </ol>
            <br>
            <img src="images/clustering_dataprep.PNG" class= "DataPrep-Step" style="max-width: 100%;">
            <br>
            <p><b>Data Filter: </b>The data was filter for store 51 and only sales data between quarter 1 to 3 were used, saving quarter 4 for testing in the later stages.</p>
            <br>
            <p>Find the SQL script used to retrive and merge data <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/Clustering/Data%20Retrival%20PostGreys%20Script.sql" target="_blank">here</a></p>
            <br>
            <h3>Experiment Setup & Results</h3>
            <h4>Data Normalization:</h4><br>
            <p>The Demand and Profit values were on different scale and were well spread out. Hence, Min-Max normalization was applied.</p>
            <br><img src="images/clustering_normalization.PNG" class= "DataPrep-Step" style="max-width: 30%;">
            <br><img src="images/clustering_normalization2.PNG" class= "DataPrep-Step" style="max-width: 100%;">
            <br>

            <h4>K-Means Clustering:</h4><br>
            <ol>
                <li>Determining the Optimal Number of Clusters:</li><br>
                <p>Both the Elbow method and Silhouette method were used to find the optimal cluster count.</p>
                <br><img src="images/clustering_optimalcluster.PNG" class= "DataPrep-Step" style="max-width: 70%;">
                <p>Both methods suggested four number of clusters. However, 3 and 5 clusters were also computed for comparison.</p>
                <br>

                <li>K-Means implementation:</li><br>
                <p>K-Means clustering with 3, 4 and 5 clusters using Euclidean distance was performed.</p>
                <iframe src="external_html/kmeans_cluster_3.html" width= "100%" height= "300px" frameborder="0"></iframe>
                <iframe src="external_html/kmeans_cluster_4.html" width="100%" height="300px" frameborder="0"></iframe>
                <iframe src="external_html/kmeans_cluster_5.html" width="100%" height="300px" frameborder="0"></iframe>
                <p>Find the data preparation and K-Means clustering Python script <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/Clustering/K-Means%20Clustering.ipynb" target="_blank">here</a></p>
                <br>
            </ol>
            <br>

            <h4>Hierarchical Clustering:</h4><br>
            <ol>
                <li>Determining the Optimal Number of Clusters:</li><br>
                <p>The Elbow method and Silhouette method were utilized to identify the optimal cluster count:</p>
                <br><img src="images/clustering_hier_optimalcluster.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
                <p>While both methods suggested two clusters, which is insufficient for this use case, attempts were made to cluster the data into 3, 4, and 5 clusters, and the results were compared.</p>
                <br>

                <li>Agglomerative clustering implementation:</li><br>
                <p>Hierarchical clusters were computed using Cosine Similarity distance metric, and Ward linkage method. The dendogram of the result is displayed below:</p>
                <br><img src="images/clustering_hier_clusters.PNG" class= "DataPrep-Step" style="max-width: 70%;">
                <img src="images/clustering_hier_dendogram.png" class= "DataPrep-Step" style="max-width: 50%;">
                <br>
            </ol>
            <br>
            <p>Find the hierarchical clustering R script <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/Clustering/Hierarchical%20Clustering.Rmd" target="_blank">here</a></p>
            <br>
            <h3>Inference & Conclusion</h3>
            <br>
            <p>Both K-Means and Hierarchical clustering have effectively organized the data, displaying a similar distribution across clusters with minimal discrepancies. Notably, cluster number 2 exhibits the highest number of records in both methods. After careful evaluation, the <b>4-cluster partition is chosen as the final clustering solution for this analysis.</b> While the 3-cluster split captures a larger proportion of points in the Low Profit Low demand cluster, 5 clusters divides the data into unnecessary chunks. In contrast, the 4-cluster split aligns better with the objective. Summary of each clusters are presented below:</p>
            <br>
            <p>The clusters represent the following:</p>
            <br><img src="images/clustering_cluster_summary.PNG" class= "DataPrep-Step" style="max-width: 50%;">
            <br>
            <h4>Cluster 1: Cash Cow- High Demand Low Profit</h4>
            <p>These are cash cow products that are very frequently purchased that yield lowest profit per product but yields the highest profit overall. These products generate consistent cash flow and are often considered as the main revenue drivers for a company. These products must have the highest inventory share.</p>
            <br>

            <h4>Cluster 0: Commodity- Low Demand Moderate Profit</h4>
            <p>These are commodity products that are typically bought less but have moderate profit leading to a decent overall profit. These products must have moderate inventory share.</p>
            <br>

            <h4>Cluster 3: Luxury- Low Demand High Profit</h4>
            <p>These are premium products that are rarely purchased but yields the highest profit. These products must have the lowest inventory share.</p>
            <br>

            <h4>Cluster 2: Laggards- Low Demand Low Profit</h4>
            <p>These products are laggards that have reasonable demand but low profits. These products usually take up the inventory space and results in low overall profit. These products must have low inventory share.</p>
            <br>
            <p>The above four clusters are well interpretable, and the cluster numbers are renamed with its appropriate names and are added as a feature “ProductDemandCluster” in the product info table.</p>
            <br>


        </div>
    </div>
    
    <div class="tab-contents" id="ARM-Content">
        <div class="intro">
            <h1 class="sub-title">Association Rule Mining</h1>
            <br><br>
            <p>Association Rule Mining (ARM) is an unsupervised machine learning method used to discover patterns and association in a transaction data. The goal is to typically identify relationships between the itemset in the format <b>{Antecedent}</b> -> <b>{Consequent}</b>. ARM is computed using three key measures:</p>
            <br><img src="images/ARM_intro_1.png" class= "DataPrep-Step" style="max-width: 70%;">
            <br>
            <ol>
                <li>Support:
                    <ul>
                        <li>It is the frequency of an item/ itemset occurred in the entire sale. It is more like the prior probability. o	Low support signifies that the product is not purchased by sufficient # people to apply association rules.</li>
                    </ul>
                </li>
                <li>Confidence:</li>
                    <ul>
                        <li>Defines the level of confidence that Consequents will be bought when Antecedent is bought already. It is more like a conditional probability. o	Low confidence signifies that consequent is independent of antecedent and no association exist between them.</li>
                    </ul>
                <li>Lift:</li>
                    <ul>
                        <li>Defines how much the probability of buying consequent is lifted when purchased along with antecedent rather just buying them independently. Always, Confidence > Support (Lift > 1) is ideal as it signifies that buying Antecedent lifts the purchase of Consequent. It is conditional probability divided by prior probability.</li>
                    </ul>
            </ol>
            <br><img src="images/ARM_intro_2.png" class= "DataPrep-Step" style="max-width: 70%;">
            <br>
            <p>Apriori stands as a widely utilized association rule mining (ARM) technique rooted in the fundamental principle: "If an itemset is frequent, then all of its subsets must also be frequent." Leveraging this property, ARM efficiently prunes item sets with low support, thereby minimizing the search space. In this project, we're using the Apriori algorithm to find connections between liquor products based on their Alcohol Type and Flavor. <b>The goal is to figure out which combinations of Alcohol Type and Flavor are frequently bought together, giving us insights into customer preferences.</b></p>
            <br>
            <h3>Data Preparation</h3><br>
            <p>Unlike other machine learning algorithms, ARM is used with unlabelled transactional data. Therefore, from the Sales_History dataset, only variables related to alcohol type and flavor profile are extracted.</p>
            <br>
            <img src="images/arm_dataprep.PNG" class= "DataPrep-Step" style="max-width: 80%;">
            <br>

            <h3>Experiment Setup & Results</h3><br>
            <p>Apriori algorithm was used to extract rules from the dataset for the below combinations:</p>
            <ul>
                <li style="list-style-type: circle">Minimum Support: 5%</li>
                <li style="list-style-type: circle">Confidence: 10%</li>
            </ul>
            <p>The item sets with top 10 support, confidence and lift are displayed below along with it's network representation.</p>
            <br>
            <img src="images/arm_toprules.PNG" class= "DataPrep-Step" style="max-width: 80%;">
            <div style="text-align: center;">
                <iframe width="800" height="600" src='external_html/ARM_network_graph.html'></iframe>
            </div>
            <img src="images/arm_rules_network.png" class= "DataPrep-Step" style="max-width: 40%;">
            <br>

            <h3>Conclusion</h3><br>
            <p>The findings indicate that Fruity flavor and Wine alcohol type are the top sellers individually, showing high support. Specifically, the combination of Fruity flavor with Wine is the most commonly purchased, displaying high confidence and lift. However, no significant relationships are observed among other combinations. This suggests that, in general, <b>people do not exhibit strong preferences for specific Alcohol Types and Flavors beyond Fruity flavor with Wine.</b></p>
            <br>
            <p>Find the ARM R script <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization-WIP-/blob/main/ARM/ARM.Rmd" target="_blank">here</a></p>
        </div>
    </div>

    <div class="tab-contents" id="NB-Content">
        <div class="intro">
            <h1 class="sub-title">Naive Bayes Classification</h1>
            <br><br>
            <p>Naive Bayes classifier is a supervised machine learning algorithm used for classification tasks. It is based on the Bayes' theorem describing the probability of a hypothesis given an evidence. Naive Bayes assumes that features are independent of each other given the class label, which is why it's called "naive."</p>
            <br><img src="images/inventory_nb_intro1.PNG" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <br><img src="images/inventory_nb_intro2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br>
            <p>There are two main types of Naive Bayes classifier available:</p>
            <br>
            <ol>
                <li>Multinomial Naive Bayes:  This type is suitable for features that represent counts or frequencies of events. It implements the naive bayes algorithm for multinomially distributed data.</li>
                <li>Bernoulli Naive Bayes: This type is suitable for binary feature variables. It assumes that each feature is binary (0 or 1).</li>
            </ol>
            <br>
            <p>In all Naive Bayes models, smoothing is often applied to prevent zero probabilities. This is to ensure more reliable predictions by adding a small value to observed counts of each feature during training.</p>

            <br><p>From the clustering results, it was determined that products could be categorized into four groups based on their profit and demand levels. While the profit of products remains relatively stable, the demand for each product fluctuates significantly on a weekly basis. Although the average demand calculated for each cluster provides an overall value, it cannot be assumed that every product will experience the same demand consistently.</p>
            <br><p>To address this issue, forecasting the demand for each product at least on a weekly level is necessary to predict future demand accurately and allocate products to the appropriate category. While using regression techniques to predict the exact demand value would be ideal, it may be a challenging task given the variability in demand. Therefore, to begin with, <b>we'll approach this as a classification problem aiming to predict the demand category (high vs low) by dividing products using a threshold demand value per week.</b> This threshold value will be determined as the minimum demand of the Cash Cow clustering category.</p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>Naive Bayes algorithms require labeled data for training. Therefore, a new label column named "Demand_Ctg" has been created. The dataset is structured at the week level, and the "Demand_Ctg" column is populated with either "Low" or "High" based on the Sales Quantity for that week divided by a predetermined threshold.</p>
            <br><img src="images/inventory_DT_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the Naive Bayes algorithm, which exclusively accepts numerical data. Hence, One-Hot encoding is employed to transform categorical data into a numerical format. Additionally, the dataset is partitioned into two disjoint sets: training and test sets at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/inventory_DT_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/19ELwAa5q6WiNq5SzeNPIsYO2Zm90beWk?usp=sharing" target="_blank">here</a></p>
            
            <h3>Experiment Setup & Results</h3><br>
            <p>A Multinomial Naive Bayes Classifier was used for classification and yeilding the following results:</p>
            <br><img src="images/inventory_nb_result1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization/blob/main/Classification/01_Classification_DataPrep_NB_DT.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>The Naive Bayes classifier's performance was poor as it did not capture the trends accurately. Though it was able to classify the results at 61% accuracy- it is still low and cannot be used to predict out-of-stock inventory for timely products purchase from the vendors.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="DT-Content">
        <div class="intro">
            <h1 class="sub-title">Decision Tree Classification</h1>
            <br><br>
            <p>Decision Tree classifier is a supervised machine learning algorithm used for classification tasks. It constructs a directional tree architecture by partitioning the data at every level and the leaf nodes are assigned a class label.</p>
            <br><img src="images/inventory_DT1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>There are infinite number of possible decision trees that can be constructed from a dataset based on the choice of split algorithm and stopping criteria.</p>
            <br><img src="images/inventory_DT2.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>There are two split algorithms that are primarily used:</p>
            <br>
            <p>1. Entropy & Information Gain:</p>
            <ul>
                <li style="list-style-type: disc;">Entropy defines the level of uncertainity in a node. More uncertainity is recorded when more variance is observed in the data.</li>
                <li style="list-style-type: disc;">Entropy values range between 0 to 1, with 0 being the best (lowest uncertainity) and 1 being the worst. Information gain is the opposite of Entropy that captures the amount of information gained after the split.</li>
                <br><img src="images/inventory_entropy.PNG" class= "DataPrep-Step" style="max-width: 100%;"><br>
            </ul>
            <p>2. Gini Index:</p>
            <ul>
                <li style="list-style-type: disc;">Gini value, like Entropy, defines the level of uncertainity in a node. More uncertainity is recorded when more variance is observed in the data.</li>
                <li style="list-style-type: disc;">Gini value in the leaf node ranges between 0 to 0.5, with 0 being the best (lowest uncertainity) and 0.5 being the worst.</li>
                <br><img src="images/inventory_gini.PNG" class= "DataPrep-Step" style="max-width: 100%;"><br>
            </ul>

            <br><p>From the clustering results, it was determined that products could be categorized into four groups based on their profit and demand levels. While the profit of products remains relatively stable, the demand for each product fluctuates significantly on a weekly basis. Although the average demand calculated for each cluster provides an overall value, it cannot be assumed that every product will experience the same demand consistently.</p>
            <br><p>To address this issue, forecasting the demand for each product at least on a weekly level is necessary to predict future demand accurately and allocate products to the appropriate category. While using regression techniques to predict the exact demand value would be ideal, it may be a challenging task given the variability in demand. Therefore, to begin with, <b>we'll approach this as a classification problem aiming to predict the demand category (high vs low) by dividing products using a threshold demand value per week.</b> This threshold value will be determined as the minimum demand of the Cash Cow clustering category.</p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>Decision Tree algorithms require labeled data for training. Therefore, a new label column named "Demand_Ctg" has been created. The dataset is structured at the week level, and the "Demand_Ctg" column is populated with either "Low" or "High" based on the Sales Quantity for that week divided by a predetermined threshold.</p>
            <br><img src="images/inventory_DT_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the Decision Tree algorithm, which exclusively accepts numerical data. Hence, One-Hot encoding is employed to transform categorical data into a numerical format. Additionally, the dataset is partitioned into two disjoint sets: training and test sets at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/inventory_DT_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/19ELwAa5q6WiNq5SzeNPIsYO2Zm90beWk?usp=sharing" target="_blank">here</a></p>

            <h3>Experiment Setup & Results</h3><br>
            <p>Three Decision Tree Classifiers were built for classification and they yielded the following results:</p>
            <br><img src="images/inventory_DT_result2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Decision Tree with Gini criterion and controlled max depth reduced overfitting and yielded the best result.</p>
            <br><img src="images/inventory_DT_result1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Find the full tree image <a href="https://drive.google.com/file/d/1TzRLMWEyUfJ--wfUVIMduAKReXBdgtu1/view?usp=sharing" target="_blank">here</a></p>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization/blob/main/Classification/01_Classification_DataPrep_NB_DT.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>The Decision Tree classifier outperformed the Naive Bayes classifier, capturing trends more effectively and achieving higher accuracy. Overall, we have developed a model capable of predicting the demand category of a product solely based on product information, selling price, and the week of analysis. This predictive capability can be leveraged to anticipate out-of-stock inventory and facilitate timely purchase products from the vendors.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="SVM-Content">
        <div class="intro">
            <h1 class="sub-title">Support Vector Machine Classification</h1>
            <br><br>
            <p>Support Vector Machine (SVM) is a supervised learning algorithm used for classification tasks. In SVM, all data, including categorical data, is represented as quantitative vectors, with each row serving as a vector. It is important to convert the data into quantitative vectors for SVM's reliance on numerical features for the computation optimal decision boundary.</p>
            <br>
            <p>The vectors located on the boundary of the separable margins are termed support vectors. Each SVM model is designed to classify between two categories, such as 0 vs 1. SVM is particularly useful for linearly separable data, where it can effectively create a boundary between classes. However, in cases where data is not linearly separable, SVM employs kernel functions to add extra dimensions to the data, making it linearly separable.</p>
            <br><img src="images/b2b_svm_intro2.PNG" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <p>Kernels use dot products to compute the similarity between pairs of data points in a high-dimensional space efficiently. This similarity measure is essential for determining the decision boundary between classes and identifying support vectors. By using the dot product in hthe higher dimentional space, the model can effectively seperate the data.</p>
            <br><img src="images/inv_svm_image4.png" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>The following two kernal functions are the most popularly used ones:</p>
            <br><img src="images/inv_svm_image2.png" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Here's an example of casting a 2D point in a higher dimension using kernal and dot products:</p>
            <br><img src="images/inv_svm_image3.png" class= "DataPrep-Step" style="max-width: 50%;"><br>

            <br><p>From the clustering results, it was determined that products could be categorized into four groups based on their profit and demand levels. While the profit of products remains relatively stable, the demand for each product fluctuates significantly on a weekly basis. Although the average demand calculated for each cluster provides an overall value, it cannot be assumed that every product will experience the same demand consistently.</p>
            <br><p>To address this issue, forecasting the demand for each product at least on a weekly level is necessary to predict future demand accurately and allocate products to the appropriate category. While using regression techniques to predict the exact demand value would be ideal, it may be a challenging task given the variability in demand. Therefore, to begin with, <b>we'll approach this as a classification problem aiming to predict the demand category (high vs low) by dividing products using a threshold demand value per week.</b></p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>In order to train SVM algorithms, labeled data is essential. Hence, a new label column called "Sentiment" has been generated. The values in the "Sentiment" column are determined by the star ratings provided. Ratings higher than 3 have been categorized as "Positive," while the remaining ratings are labeled as "Negative."</p>
            <br><img src="images/b2b_classification_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the SVM algorithm, which exclusively accepts data that are partitioned into two disjoint sets: training and test sets. Hence, the dataset has been split at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/b2b_classification_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/1SK_nSuh8YEwTltD1Uq_kFClbBOQdk14O?usp=drive_link" target="_blank">here</a></p>
            <br><p>Find the data preparation code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Data_Preparation.ipynb" target="_blank">here</a></p>

            <h3>Experiment Setup & Results</h3><br>
            <p>Three combinations of SVM kernel and cost parameter were used for classification and they yielded the following results:</p>
            <br><img src="images/inv_svm_image5.png" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>All three combinations yeilded moreover the same result.</p>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/Inventory-Optimization/blob/main/Classification/02_ClassificationSVM.ipynb " target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>In comparison to the Decision Tree algorithms, the SVM has displayed a poor performance. Overall, we have developed a model capable of predicting the demand category of a product solely based on product information, selling price, and the week of analysis. This predictive capability can be leveraged to anticipate out-of-stock inventory and facilitate timely purchase products from the vendors.</p>
            <br>
        </div>
    </div>


    <div class="tab-contents" id="Conclusion-Content">
        <div class="intro">
            <h1 class="sub-title">Conclusion</h1>
            <br><br>
            <p>This project tackled the challenge of overstocking and understocking in retail stores. A Machine Learning model was designed to predict ideal purchase timing and quantities, preventing these issues. This solution empowers retailers to:</p>
            <br>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Forecast demands a week in advance:</b> This allows for strategic purchase planning, considering vendor fulfillment time.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Optimize inventory management:</b> By understanding product categories based on profit and demand (cash cows, laggards, commodities, luxury items), retailers can prioritize high-demand, high-profit items (cash cows) while minimizing stock of low-demand, low-profit items (laggards). This ensures they have the right products available to meet customer needs and maximize profits.</li>
                </ul>
            <br>
            <br><img src="images/inv_conclusion1.png" class= "DataPrep-Step" style="max-width: 70%;"><br>

            <br><p>Three crucial factors were identified for calculating ideal purchase quantities and timing:</b></p>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Inventory Level:</b> Existing stock helps determine how much to buy.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Demand Forecast:</b> The model predicts demand for the next week, ensuring enough products are available.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Fulfillment Time:</b> Vendor lead time is factored in to avoid stockouts.</li>
            </ul>

            <br>
            <h3>Results and Potential</h3><br>
            <br><p>Using these factors, the model forecasted demand, calculated potential overstock/understock, and compared it to actual inventory levels. This allowed for estimation of:</p>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Cost Savings:</b> The model predicted potential savings of $8,000 and reduced storage needs for 300 products across just four unique items in the final ten weeks.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Scalability:</b> Projecting these savings to a larger store network and product range suggests significant financial and space optimization benefits.</li>
            </ul>
            <br><img src="images/inv_conclusion2.png" class= "DataPrep-Step" style="max-width: 70%;"><br>


            <h3>Call to Action</h3><br>
            <br><p>This project demonstrates the power of data-driven inventory management. By adopting this approach, retailers can achieve:</p>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Reduced Costs:</b> Lower storage expenses and minimized overstocking waste.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Increased Sales:</b> Improved stock availability to meet customer demand.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Enhanced Efficiency:</b> Streamlined purchasing and optimized storage utilization.</li>
            </ul>

            <h3>Next Steps</h3><br>
            <br><p>While the model addresses key factors, further enhancements are possible:</p>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Inventory Space Integration:</b> Including available storage space in calculations can ensure efficient use of resources.</li>
                <li style="list-style-type: circle; color: #c0c0c0;"><b>Fulfillment Cost Consideration:</b> Factoring in vendor fulfillment costs can lead to a more cost-effective solution.</li>
            </ul>
            <br><p>By continuously refining this model, retailers can achieve a "bulletproof" solution for inventory management, leading to a more successful and profitable business.</p>

            
        </div>
    </div>


</div>

<div class="copyright" id="copyright">
</div>


<script>

    var tablinks = document.getElementsByClassName("tab-links");
    var tabcontents = document.getElementsByClassName("tab-contents");

    function opentab(tabname){
        for(tablink of tablinks){
            tablink.classList.remove("active-link");
        }
        for(tabcontent of tabcontents){
            tabcontent.classList.remove("active-tab");
        }
        event.currentTarget.classList.add("active-link");
        document.getElementById(tabname).classList.add("active-tab");
    }

</script>

<script>

    var sidemeu = document.getElementById("sidemenu");

    function openmenu(){
        sidemeu.style.right = "0";
    }
    function closemenu(){
        sidemeu.style.right = "-200px";
    }

</script>
<script>
    const scriptURL = '< add you own link here >' // add your own app script link here
    const form = document.forms['submit-to-google-sheet']
    const msg = document.getElementById("msg")
  
    form.addEventListener('submit', e => {
      e.preventDefault()
      fetch(scriptURL, { method: 'POST', body: new FormData(form)})
        .then(response => {
            msg.innerHTML = "Message sent successfully"
            setTimeout(function(){
                msg.innerHTML = ""
            },5000)
            form.reset()
        })
        .catch(error => console.error('Error!', error.message))
    })
  </script>
</body>
</html>