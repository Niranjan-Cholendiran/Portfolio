<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Niranjan's Portfolio</title>
    <link rel="stylesheet" href="style_project_2.css">
    <script src="https://kit.fontawesome.com/90171aec82.js" crossorigin="anonymous"></script>
</head>
<body>
    
<div id="header">
    <div class="container">
        <nav>
            <a href="index.html#header"><img src="images/logo.png" class="logo"></a>
            <ul class="menu" id="sidemenu">
                <li><a href="index.html#header">Home</a></li>
                <li><a href="index.html#about">About Me</a></li>
                <li><a href="index.html#projects">My Work</a>
                        <div class="sub-menu-1"> 
                            <ul>

                                <li class="sub-menu-2-hover"><a href="project1.html#header">Inventory Optimization (CSCI 5622)</a>
                                    <div class="sub-menu-2">
                                        <ul>
                                            <li><a href="project1.html#Introduction-Content">Introduction</a></li>
                                            <li><a href="project1.html#navigation-content">DataPrep_EDA</a></li>
                                            <li class="sub-menu-3-hover"><a href="project1.html#navigation-content">Modeling</a>
                                                <div class="sub-menu-3">
                                                    <ul>
                                                        <li><a href="project1.html#navigation-content">Clustering</a></li>
                                                        <li><a href="project1.html#navigation-content">ARM</a></li>
                                                        <li><a href="project1.html#navigation-content">Naïve Bayes</a></li>
                                                        <li><a href="project1.html#navigation-content">Decision Tree</a></li>
                                                        <li><a href="project1.html#navigation-content">SVMs</a></li>
                                                        <li><a href="project1.html#navigation-content">Regression</a></li>
                                                        <li><a href="project1.html#navigation-content">Neural Nets</a></li>
                                                    </ul>
                                                </div>                                        
                                            </li>
                                        </ul>
                                    </div>
                                </li>


                                <li class="sub-menu-2-hover"><a href="project2.html#header">B2C Brand Perception (INFO 5871)</a>
                                    <div class="sub-menu-2">
                                        <ul>
                                            <li><a href="#Introduction-Content" onclick="opentab('Introduction-Content')">Introduction</a></li>
                                            <li><a href="#Data-Preparation-Content" onclick="opentab('Data-Preparation-Content')">DataPrep</a></li>
                                            <li class="sub-menu-3-hover"><a href="#Clustering-Content" onclick="opentab('Clustering-Content')">Modeling</a>
                                                <div class="sub-menu-3">
                                                    <ul>
                                                        <li><a href="#Clustering-Content" onclick="opentab('Clustering-Content')">Clustering</a></li>
                                                        <li><a href="#LDA-Content" onclick="opentab('LDA-Content')">LDA</a></li>
                                                        <li><a href="#ARM-Content" onclick="opentab('ARM-Content')">ARM</a></li>
                                                        <li><a href="#NB-Content" onclick="opentab('NB-Content')">Naïve Bayes</a></li>
                                                        <li><a href="#DT-Content" onclick="opentab('DT-Content')">Decision Tree</a></li>
                                                        <li><a href="#SVM-Content" onclick="opentab('SVM-Content')">SVM</a></li>
                                                        <li><a href="#NN-Content" onclick="opentab('NN-Content')">Neural Network</a></li>
                                                        <li><a href="#Conclusion-Content" onclick="opentab('Conclusion-Content')">Conclusion</a></li>
                                                    </ul>
                                                </div>
                                            </li>
                                        </ul>
                                    </div>
                                </li>


                                <li class="sub-menu-2-hover"><a href="project3.html#header">Dock Watch (CSCI 5502)</a>
                                    <div class="sub-menu-2">
                                        <ul>
                                            <li><a href="project3.html#Introduction-Content">Introduction</a></li>
                                            <li><a href="project3.html#navigation-content">Data Preparation</a></li>
                                            <li><a href="project3.html#navigation-content">Data Exploration</a></li>
                                            <li><a href="project3.html#navigation-content">Modeling</a></li>
                                        </ul>
                                    </div>
                                </li>


                            </ul>
                        </div>                
                    </li>               
                </li>
                <li><a href="index.html#contact">Contact</a></li>
                <i class="fas fa-times" onclick="closemenu()"></i>
            </ul>
            <i class="fas fa-bars" onclick="openmenu()"></i>
        </nav>
        <div class="header-text" style="padding-top: 3%;">
            <h1>B2C Brand Perception Analysis</h1>
        </div>
    </div>
</div>

<!-- -----------Project Contents---------- -->
<div class="container" id="navigation-content">
    <div class="tab-titles" >
        <p class="tab-links active-link" onclick="opentab('Introduction-Content')">Introduction</p>
        <p class="tab-links" onclick="opentab('Data-Preparation-Content')">Data Preparation</p>
        <p class="tab-links" onclick="opentab('Clustering-Content')">Clustering</p>
        <p class="tab-links" onclick="opentab('LDA-Content')">LDA</p>
        <p class="tab-links" onclick="opentab('ARM-Content')">ARM</p>
        <p class="tab-links" onclick="opentab('NB-Content')">Naïve Bayes</p>
        <p class="tab-links" onclick="opentab('DT-Content')">Decision Tree</p>
        <p class="tab-links" onclick="opentab('SVM-Content')">SVM</p>
        <p class="tab-links" onclick="opentab('NN-Content')">Neural Network</p>
        <p class="tab-links" onclick="opentab('Conclusion-Content')">Conclusion</p>

    </div>

    <div class="tab-contents active-tab" id="Introduction-Content">
        <div class="intro">
            <h1 class="sub-title">Introduction</h1>
            <br><br>
            <p>On an average, a company allocates approximately 9.1% of its revenue to marketing (<a href="https://vitaldesign.com/percent-of-revenue-spent-on-marketing-sales/#:~:text=Businesses%20large%20and%20small%20understand,12.1%25%20across%20industries%20in%202016." target="_blank">source</a>), with a significant portion directed towards campaign planning & content creation (40-50%), paid advertising (20-30%), and the rest on workforce marketing and events. (<a href="https://business.adobe.com/blog/basics/a-high-level-b2b-marketing-budget-breakdown" target="_blank">source</a>). But what is the purpose of investing not only so much money, but also a lot of time and effort, into marketing?</p>
            <br>
            <p>The current market is vast, and regardless of the industry, every company faces thousands of competitors. In such an era, it is crucial to stay ahead and build a good perception among consumers in order to succeed. One of the best ways to understand the importance of brand perception is through McKinsey’s Consumer Decision Journey chart:</p>
            <br>
            <img src="images/b2c_intro1.png" class= "intro-quote">
            <br>
            <P>This chart clearly shows the decision cycle of a consumer starting from the consideration to brand advocacy. While each state is crucial, the Evaluation and Advocacy stage plays an upper hand:</P>
            <br>
            <ul>
                <li style="list-style: circle;">Evaluation stage is when consumers research to make an informed decision. Where they collect information from their own, ask friends, read public opinions, etc and make a final decision to engage further or not.</li>
                <li style="list-style: circle;">Advocacy stage is when a consumer gets into the loyalty loop and advocate for the brand’s services/ products.</li>
            </ul>
            <br>
            <p>These 2 stages play an important role in a company’s success, retaining existing customers and gaining new customers as word of mouth is the most powerful source for any customer. Therefore, significant marketing efforts has been put globally to ensure a brand has a loyal set of customers advocating for it.</p>
            <br>
            <img src="images/b2c_intro2.png" class= "intro-quote">
            <br>
            <p>Now that we know it is important, we can also realize that it is importance of measuring a brand’s perception, right? But the question is how?</p>
            <br>
            <p>In the past, measuring brand perception was challenging and required a lot of manual investigation. However, in today’s digital age, where customers opinions are readily available all over the internet. It is easy to find both sided perspective about a any brand online. So, if one can harness the power of these opinions by building a framework capable of collecting data about a brand across the web, analysing them and providing insights on to the area of improvement, it’d be a success for the company to measure it’s customer perception and tailor their marketing strategies according.</p>
            <br>
            <p>However, it's important to note that most consumers tend to write reviews when they are dissatisfied. Therefore, there’s a high possibility that the data collected from the online sites may not represents both sides equally. Nevertheless, the primary goal remains understanding the customer's point of view and identifying opportunities for improvement, making the skewed data less significant.</p>
            <br>
            <p>Ultimately, this framework can be used to measure:</p>
            <ul>
                <li style="list-style: circle;">Brand's reach based on number of reviews.</li>
                <li style="list-style: circle;">Brand's perception through customer review sentiment.</li>
                <li style="list-style: circle;">Brand's perception across various categories such as quality, customer support, shipping, returns, service, value for money, etc.</li>
                <li style="list-style: circle;">Brand perception shift over time, especially after a major marketing campaign or decision.</li>
            </ul>
        </div>
    </div>

    <div class="tab-contents" id="Data-Preparation-Content">
        <div class="intro">
            <h1 class="sub-title">Data Collection</h1>
            <br><br>
            <p>Though the goal is to build a dynamic framework capable of analyzing the brand perception of any chosen company. To construct and validate this framework, an initial sample dataset has been compiled from various sources. The chosen sample is a well-recognized ride-hailing service company, "<b>Uber</b>" and the collected data is a combination of customer reviews, recent news articles, and Reddit posts. Find more information regarding the data collection process below. </p>
            <br><br>

            <h3>Source 1: Trustpilot</h3>
            <br><br>
            <p>Trustpilot is a free public review platform featuring diverse reviews from customers. Data from Trustpilot were <b>web scrapped using the Beautiful Soup python library</b>. In addition to customer reviews, the following information were also scrapped from the website to ensure the analysis be presented across multiple dimensions:</p>
            <br>
            <ol>
                <li>Reviewer Name</li>
                <li>Reviewer Total Reviews</li>
                <li>Location</li>
                <li>Review Star Rating</li>
                <li>Review Title</li>
                <li>Review</li>
                <li>Date</li>
                <li>Review Likes</li>
            </ol>
            <img src="images/b2c_source1.PNG" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
            <br>
            <p>Find the data collection code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/01_Data_Collection/01_TrustPilot_WebScrape.ipynb" target="_blank">here</a></p>
            <p>Download the raw data <a href="https://drive.google.com/drive/folders/1ePnv1q3zAfQiU-iUNK3ku6prUV0-J5Kl?usp=sharing" target="_blank">here</a></p>
            <br>


            
            <h3>Source 2: NewsAPI</h3>
            <br><br>
            <p>While customer reviews offer a great source to measure brand perception. It is also crucial to keep track of public news, especially the popular ones, to analyse the current pulse of market and potential areas of improvement. Hence, NewsAPI were used to collect recent popular news articles tagged on the company “Uber”. The data were collected on the following topics individually:</p>
            <br>
            <ol>
                <li>Customer Service</li>
                <li>Safety</li>
                <li>Promotions</li>
                <li>Pricing</li>
            </ol>
            
            <br>
            <h4>API Components:</h4>
            <br>
            <ul>
                <li>1. Endpoint: <i>https://newsapi.org/v2/everything</i></li>
                <li>2. Parameters:</li>
                <ul>
                    <li>q: <i>uber%20NOT%20eats%20AND%20[topic]</i></li>
                    <li>language: <i>en</i></li>
                    <li>apiKey: <i>[API KEY]</i></li>
                    <li>sortby: <i>popularity</i></li>
                </ul>
            </ul>
            <img src="images/b2c_source2.PNG" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
            <br>
            <p>Find the data collection code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/01_Data_Collection/02_NewsAPI_Data_Collection.ipynb" target="_blank">here</a></p>
            <p>Download the raw data <a href="https://drive.google.com/drive/folders/1ePnv1q3zAfQiU-iUNK3ku6prUV0-J5Kl?usp=sharing" target="_blank">here</a></p>
            <br>

            <h3>Source 3: Reddit</h3>
            <br><br>
            <p>In addition to customer reviews and news articles, social network contents from Reddit was also collected to monitor the company’s campaigns and promotions- which could potentially influence and shift customers perception. Reddit API were used for this purpose.</p>
            <br>
            <h4>API Components:</h4>
            <br>
            <ul>
                <li>1. Endpoint: <i>https://oauth.reddit.com/r/[subreddit]/new</i></li>
                <li>2. Parameters:</li>
                <ul>
                    <li>subreddit: <i>Uber</i></li>
                    <li>g: <i>en</i></li>
                    <li>authentication: <i>[AUTHENTICATION TOKEN]</i></li>
                    <li>sortby: <i>popularity</i></li>
                </ul>
            </ul>
            <img src="images/b2c_source3.PNG" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
            <br>
            <p>Find the data collection code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/01_Data_Collection/03_Reddit_Auth_and_Data_Collection.ipynb" target="_blank">here</a></p>
            <p>Download the raw data <a href="https://drive.google.com/drive/folders/1ePnv1q3zAfQiU-iUNK3ku6prUV0-J5Kl?usp=sharing" target="_blank">here</a></p>



            <br><br>
            <h1 class="sub-title">Data Preparation</h1>
            <br><br>
            <p>The collected raw data contains a combination of text, categorical and numerical features. However, it cannot be used directly for analysis, especially the text data. Hence, the data has been pre-processed in a was that can be used for modeling. The following are a few data cleaning steps performed on the raw data.</b></p>
            <br>
                
            <h4>Data Preparation Steps:</h4>
            <ul>
                <li>1. Unique identifier creation:</i>
                    <ul>
                        <li>Every review was tagged to an unique ID to ensure traceability throughout the analysis.</li>
                    </ul>
                    <img src="images/b2c_dataprep1.PNG" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
                </li>
                <br>

                <li>2. Filtering:</i>
                    <ul>
                        <li>The dataset includes reviews from around the world dating back over 10 years. To focus on recent feedback and ensure relevance, the data is filtered out for the last 3 years from the United States.</li>
                    </ul>
                </li>
                <br>

                <li>3. Removed special characters and numbers:</i>
                    <ul>
                        <li>Along with texts, the reviews were a mix of punctuations, emojis and numbers, which are not required in this use case. Hence, they were removed from the dataset.</li>
                    </ul>
                    <img src="images/b2c_dataprep3.png" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
                </li>
                <br>

                <li>4. Lemmatization:</i>
                    <ul>
                        <li>The words were lemmatized to ensure different forms of a same word are treated identically</li>
                    </ul>
                    <img src="images/b2c_dataprep4.png" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
                </li>
                <br>

                <li>5. Stop words Removal:</i>
                    <ul>
                        <li>The reviews included words that lacked contextual relevance; hence they were removed from the texts. Along with NLTK’s inbuilt stop words, a few self-defined stop words like “Uber” and “Driver” were also removed.</li>
                    </ul>
                    <img src="images/b2c_dataprep5.png" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
                </li>
                <br>
                <li>6. TFIDF Vectorization:</i>
                    <ul>
                        <li>Finally, the words have been vectorized by calculating the frequency and normalizing them.</li>
                        <li>Max_df of 0.7 has been applied ensure removal of content specific stop words.</li>
                    </ul>
                    <img src="images/b2c_dataprep6.png" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
                </li>
            </ul>
                <br>
        
            <h4>Word Cloud before and after cleaning:</h4>
                <img src="images/b2c_dataprep7.png" style="max-width: 90%; align-items:center; height: auto;margin: 0 auto; display: block; " >
        <br>
        <p>Find the data preprocessing code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/01_Data_Preparation/Data_Cleaning.ipynb" target="_blank">here</a></p>
        <p>Download the preprocessed data <a href="https://drive.google.com/drive/folders/1LRSoqRIfiOYIubV9_JjoJozAj-UUQzzV?usp=drive_link" target="_blank">here</a></p>

        </div>
    </div>

    <div class="tab-contents" id="Clustering-Content">
        <div class="intro">
            <h1 class="sub-title">Clustering</h1>
            <br><br>
            <p>Clustering is as an unsupervised learning method in machine learning, dividing data based on feature distances. Its goal is to group similar records within clusters (cohesion) while maintaining dissimilarity from objects in other clusters (separation). There are multiple ways to implement clustering however the most popular implementation includes K-Means and Hierarchical Clustering.</p>
            <br>
            <p><b>K-Means Clustering: </b>This partitioning technique clusters data into a set number of clusters by aggregating the K closest points to a designated point. Various distance metrics (Euclidean, Manhattan, etc.) can be utilized based on the context.</b></p>
            <img src="images/clustering_kmeans.png" class= "DataPrep-Step" style="max-width: 65%;">
            <br>
            <p><b>Hierarchical Clustering: </b>This method organizes data hierarchically, constructing dendrogram tree structures. Agglomerative clustering starts with the assumption that each point forms its own cluster, gradually merging them until ideally only one cluster remains. Similar to K-Means, diverse distance metrics and linkage methods are available, chosen according to the specific case.</b></p>
            <img src="images/clustering_hier.png" class= "DataPrep-Step" style="max-width: 30%;">
            <br>
            <p>For this application, both clustering and hierarchical clustering are applied to group similar review based on the frequency of common words.</b></p>
            <br>
            <h3>Data Preparation</h3>
            <br><br>
            <p>Both K-Means and Hierarchical clustering needs numerical data to cluster. Therefore, the document term matrix that was preprepared in the previous section was used. The document term matrix is a review number- document matrix with the values as TFIDF normalized frequencies.</b></p>
            <br>
            <img src="images/b2c_docterm.PNG" class= "DataPrep-Step" style="max-width: 100%;">
            <br>
            <p>Download the data <a href="https://drive.google.com/file/d/1oIyPIhHV1HQ244Q_wC0VUB12GXZVAb0s/view?usp=sharing" target="_blank">here</a></p>
            <br>
            <h3>Experiment Setup & Results</h3>
            <br>

            <h4>Hierarchical Clustering:</h4><br>
            <ol>
                <li>Determining the Optimal Number of Clusters:</li><br>
                <p>The Elbow method and Silhouette method were utilized to identify the optimal cluster count:</p>
                <br><img src="images/b2c_hclust_optimalclusters.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
                <p>Both methods suggested two clusters as optimal.</p>
                <br>

                <li>Agglomerative clustering implementation:</li><br>
                <p>Hierarchical clusters were computed using Cosine Similarity distance metric, and Ward linkage method. The dendogram of the result is displayed below:</p>
                <br>
                <img src="images/b2c_hclust_dendo.png" class= "DataPrep-Step" style="max-width: 80%;">
                <br>
            </ol>
            <br>
            <p>Find the hierarchical clustering R script <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/02_Clustering/Hierarchical_Clustering.Rmd" target="_blank">here</a></p>
            <br>

            <h4>K-Means Clustering:</h4><br>
            <ol>
                <li>Determining the Optimal Number of Clusters:</li><br>
                <p>Both the Elbow method and Silhouette method were used to find the optimal cluster count.</p>
                <br><img src="images/b2c_kmeans_optimalclusters.PNG" class= "DataPrep-Step" style="max-width: 70%;">
                <p>Both methods suggested two number of clusters. However, 3 and 4 clusters were also computed for comparison.</p>
                <br>

                <li>K-Means implementation:</li><br>
                <p>K-Means clustering with 2, 3 and 4 clusters using Euclidean distance was performed and the word clouds of each clusters were computed.</p>
                <br>
                <br><img src="images/b2c_kmeans_results1.PNG" class= "DataPrep-Step" style="max-width: 100%;">
                <br><img src="images/b2c_kmeans_results2.PNG" class= "DataPrep-Step" style="max-width: 100%;">

                <p>Find the K-Means clustering Python script <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/02_Clustering/K-Means.ipynb" target="_blank">here</a></p>
                <br>
            </ol>
            <br>
            
            <h3>Inference & Conclusion</h3>
            <br>
            <p>Both K-Means and Hierarchical clustering have effectively organized the data, displaying a similar distribution across clusters with minimal discrepancies. Almost all clusters have overlapping words due to the high frequency of selected words in the corpus. <b>This is a clear indicator that clustering reviews only based on similar words is insufficient.</b> Hence, a topic modeling alogrithm has been explored in the next section.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="LDA-Content">
        <div class="intro">
            <h1 class="sub-title">Latent Dirichlet Allocation (LDA)</h1>
            <br><br>
            <p>LDA is a topic modeling algorithm that is based on probabilistic modeling. In short, it assumes that each document in a corpus is a mixture of various topics, and each topic is a distribution over words. The goal of LDA is to discover these underlying topics based on the words that appear in the documents. In this project, the aim is to group reviews based on the underlying topics. </p> 

            <br><img src="images/b2c_lda_intro.PNG" class= "DataPrep-Step" style="max-width: 70%;">
            <br>

            <h3>Data Preparation</h3>
            <br><br>
            <p>LDA needs frequency of words data. Therefore, the document term matrix that was preprepared in the previous sections was used. The document term matrix is a review number- document matrix with the values as TFIDF normalized frequencies.</b></p>
            <br>
            <img src="images/b2c_docterm.PNG" class= "DataPrep-Step" style="max-width: 100%;">
            <br>
            <p>Download the data <a href="https://drive.google.com/file/d/1oIyPIhHV1HQ244Q_wC0VUB12GXZVAb0s/view?usp=sharing" target="_blank">here</a></p>
            <br>


            <h3>Experiment Setup & Results</h3>
            <br>
            <p>LDA for 2, 3, 4 and 5 topics were performed for comparison. The results of each experiment is listed below:</p>
            <img src="images/b2c_lda_topics_results.PNG" class= "DataPrep-Step" style="max-width: 80%;">
            <br>
            <p>Out of the above four topic modelling results, the one with 4 topics seems reasonable. Find the detailed result below:</p>

            <div style="text-align: center;">
                <iframe width="1200" height="600" src='external_html/b2c_lda_output.html'></iframe>
            </div>




            <br>
            <p>Find the LDA Python script <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/03_LDA/01.%20LDA_Code.ipynb" target="_blank">here</a></p>
            <br>
            <h3>Conclusion</h3><br>
            <p>Out of the above four topic modelling results, the one with 4 topics seems reasonable. Find the </p>
            <br>

            <b>Topic 1: Service Quality and Comfort</b>
            <ul><li>This topic may be discussing the quality of service provided by Uber drivers, focusing on aspects like cleanliness of vehicles, friendliness of drivers, professionalism, and overall comfort of the ride experience.</li></ul>

            <b>Topic 2: Issues with Service and Payments</b>
            <ul><li>This topic may be discussing the quality of service provided by Uber drivers, focusing on aspects like cleanliness of vehicles, friendliness of drivers, professionalism, and overall comfort of the ride experience.</li></ul>

            <b>Topic 3: Feedback on Convenience and Reliability</b>
            <ul><li>This topic likely discusses feedback on the convenience and reliability of Uber service, including discussions about fast and smooth rides, scheduling options, transportation choices, and experiences outside or on the road.</li></ul>

            <b>Topic 4: Positive Experience and Appreciation</b>
            <ul><li>This topic likely revolves around positive experiences and appreciation for Uber service, highlighting ease of use, excellent service, friendliness of drivers, and overall satisfaction with the experience.</li></ul>
            <br>
            <br>
            <p><b>Next Steps:</b> Alternatelively, a method to achieve topic modeling based on the topic of interest rather than emotions can be explored. For example,</p>
            <ul>   
                <li>Sentence 1: "I was happy about the early pickup"</li>
                <li>Sentence 2: "I was happy about how the car was comfortable"</li>
                <li>Sentence 3: "I hated the ride. The ride was discomforting."</li>
            </ul>
            <p>Sentence 1 and 2 should not be grouped (because they have same emotion), rather 2 and 3 should be together because they talk about ride quality. This could be achieved by removing the sentiment related words prior to LDA. NLTK's SentimentIntensityAnalyzer could be a good start.</p>
        </div>
    </div>

    <div class="tab-contents" id="ARM-Content">
        <div class="intro">
            <h1 class="sub-title">Association Rule Mining</h1>
            <br><br>
            <p>Association Rule Mining (ARM) is an unsupervised machine learning method used to discover patterns and association in a transaction data. The goal is to typically identify relationships between the itemset in the format <b>{Antecedent}</b> -> <b>{Consequent}</b>. ARM is computed using three key measures:</p>
            <br><img src="images/ARM_intro_1.png" class= "DataPrep-Step" style="max-width: 70%;">
            <br>
            <ol>
                <li>Support:
                    <ul>
                        <li>It is the frequency of an item/ itemset occurred in the entire sale. It is more like the prior probability. o	Low support signifies that the product is not purchased by sufficient # people to apply association rules.</li>
                    </ul>
                </li>
                <li>Confidence:</li>
                    <ul>
                        <li>Defines the level of confidence that Consequents will be bought when Antecedent is bought already. It is more like a conditional probability. o	Low confidence signifies that consequent is independent of antecedent and no association exist between them.</li>
                    </ul>
                <li>Lift:</li>
                    <ul>
                        <li>Defines how much the probability of buying consequent is lifted when purchased along with antecedent rather just buying them independently. Always, Confidence > Support (Lift > 1) is ideal as it signifies that buying Antecedent lifts the purchase of Consequent. It is conditional probability divided by prior probability.</li>
                    </ul>
            </ol>
            <br><img src="images/ARM_intro_2.png" class= "DataPrep-Step" style="max-width: 70%;">
            <br>
            <p>Apriori stands as a widely utilized association rule mining (ARM) technique rooted in the fundamental principle: "If an itemset is frequent, then all of its subsets must also be frequent." Leveraging this property, ARM efficiently prunes item sets with low support, thereby minimizing the search space. In this project, we're using the Apriori algorithm to find the combination of words that occurs together frequently in the reviews. <b>The goal is to figure out which word's frequency is lifted by other words, giving us insights into customer's perception on each topics.</b></p>
            <br>
            <h3>Data Preparation</h3><br>
            <p>Unlike other machine learning algorithms, ARM is used with unlabelled transactional data. Hence, the reviews are preprocessed by removing stop words and lemmatizing them, and converted to basket format.</p>
            <br>
            <img src="images/b2c_arm_dataprep.PNG" class= "DataPrep-Step" style="max-width: 80%;">
            <br>
            <p>Find the data preparation Python script <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/04_ARM/basket_data_prep.ipynb" target="_blank">here</a></p>
            <p>Download the data <a href="https://drive.google.com/file/d/1vwLcIdwYb6EYWwWCFdPutHpPSanpjPiN/view?usp=sharing" target="_blank">here</a></p>
            <br>

            <h3>Experiment Setup & Results</h3><br>
            <p>Apriori algorithm was used to extract rules from the dataset for the below combinations:</p>
            <ul>
                <li style="list-style-type: circle">Minimum Support: 0.3%</li>
                <li style="list-style-type: circle">Confidence: 0.5%</li>
            </ul>
            <p>The item sets with top 10 support, confidence and lift are displayed below along with it's network representation.</p>
            <br>
            <img src="images/b2c_arm_results.PNG" class= "DataPrep-Step" style="max-width: 80%;">
            <div style="text-align: center;">
                <iframe width="800" height="600" src='external_html/b2c_arm_interactive_plot.html'></iframe>
            </div>
            <img src="images/b2c_arm_network_graph.png" class= "DataPrep-Step" style="max-width: 50%;">
            <br>
            <p>Find the ARM implementation R script <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/04_ARM/ARM.Rmd" target="_blank">here</a></p>
            <br>

            <h3>Conclusion</h3><br>
            <p>ARM results are not straight forward and not every rule represents a topic, however it indicates that many customers have written about the cleanliness of the vehicle and customer service.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="NB-Content">
        <div class="intro">
            <h1 class="sub-title">Naive Bayes Classification</h1>
            <br><br>
            <p>Naive Bayes classifier is a supervised machine learning algorithm used for classification tasks. It is based on the Bayes' theorem describing the probability of a hypothesis given an evidence. Naive Bayes assumes that features are independent of each other given the class label, which is why it's called "naive."</p>
            <br><img src="images/inventory_nb_intro1.PNG" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <br><img src="images/inventory_nb_intro2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br>
            <p>There are two main types of Naive Bayes classifier available:</p>
            <br>
            <ol>
                <li>Multinomial Naive Bayes:  This type is suitable for features that represent counts or frequencies of events. It implements the naive bayes algorithm for multinomially distributed data.</li>
                <li>Bernoulli Naive Bayes: This type is suitable for binary feature variables. It assumes that each feature is binary (0 or 1).</li>
            </ol>
            <br>
            <p>In all Naive Bayes models, smoothing is often applied to prevent zero probabilities. This is to ensure more reliable predictions by adding a small value to observed counts of each feature during training.</p>

            <br><p>Having analyzed the LDA results, we've identified the key themes discussed in each review. Now, our focus shifts to understanding the underlying sentiment or emotion associated with these themes. This crucial step requires the development of a classification model capable of accurately discerning the sentiment expressed in each review.</p>
            <br><p>To achieve this, we've leveraged customers' star ratings in conjunction with their corresponding reviews as the foundational elements for our classification task. Our objective is clear: <b>we aim to construct a robust classification model that effectively predicts the sentiment conveyed within each review, distinguishing between positive and negative sentiments.</b>.</p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>In order to train Naive Bayes algorithms, labeled data is essential. Hence, a new label column called "Sentiment" has been generated. The values in the "Sentiment" column are determined by the star ratings provided. Ratings higher than 3 have been categorized as "Positive," while the remaining ratings are labeled as "Negative."</p>
            <br><img src="images/b2b_classification_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the Naive Bayes algorithm, which exclusively accepts data that are partitioned into two disjoint sets: training and test sets. Hence, the dataset has been split at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/b2b_classification_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/1SK_nSuh8YEwTltD1Uq_kFClbBOQdk14O?usp=drive_link" target="_blank">here</a></p>
            <br><p>Find the data preparation code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Data_Preparation.ipynb" target="_blank">here</a></p>

            <h3>Experiment Setup & Results</h3><br>
            <p>A Multinomial Naive Bayes Classifier was used for classification and yeilding the following results:</p>
            <br><img src="images/b2c_nb_result1.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Classification.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>
                The Naive Bayes classifier exhibited exceptional performance, accurately capturing trends and sentiment words. Achieving an impressive accuracy rate of 93%, this model stands as a reliable tool for accurately classifying the sentiment of future, unknown reviews. However, a few other classification models have been tested in the following sections.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="DT-Content">
        <div class="intro">
            <h1 class="sub-title">Decision Tree Classification</h1>
            <br><br>
            <p>Decision Tree classifier is a supervised machine learning algorithm used for classification tasks. It constructs a directional tree architecture by partitioning the data at every level and the leaf nodes are assigned a class label.</p>
            <br><img src="images/inventory_DT1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>There are infinite number of possible decision trees that can be constructed from a dataset based on the choice of split algorithm and stopping criteria.</p>
            <br><img src="images/inventory_DT2.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>There are two split algorithms that are primarily used:</p>
            <br>
            <p>1. Entropy & Information Gain:</p>
            <ul>
                <li style="list-style-type: disc;">Entropy defines the level of uncertainity in a node. More uncertainity is recorded when more variance is observed in the data.</li>
                <li style="list-style-type: disc;">Entropy values range between 0 to 1, with 0 being the best (lowest uncertainity) and 1 being the worst. Information gain is the opposite of Entropy that captures the amount of information gained after the split.</li>
                <br><img src="images/inventory_entropy.PNG" class= "DataPrep-Step" style="max-width: 100%;"><br>
            </ul>
            <p>2. Gini Index:</p>
            <ul>
                <li style="list-style-type: disc;">Gini value, like Entropy, defines the level of uncertainity in a node. More uncertainity is recorded when more variance is observed in the data.</li>
                <li style="list-style-type: disc;">Gini value in the leaf node ranges between 0 to 0.5, with 0 being the best (lowest uncertainity) and 0.5 being the worst.</li>
                <br><img src="images/inventory_gini.PNG" class= "DataPrep-Step" style="max-width: 100%;"><br>
            </ul>

            <br><p>Having analyzed the LDA results, we've identified the key themes discussed in each review. Now, our focus shifts to understanding the underlying sentiment or emotion associated with these themes. This crucial step requires the development of a classification model capable of accurately discerning the sentiment expressed in each review.</p>
            <br><p>To achieve this, we've leveraged customers' star ratings in conjunction with their corresponding reviews as the foundational elements for our classification task. Our objective is clear: <b>we aim to construct a robust classification model that effectively predicts the sentiment conveyed within each review, distinguishing between positive and negative sentiments.</b>.</p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>In order to train Decision Tree algorithms, labeled data is essential. Hence, a new label column called "Sentiment" has been generated. The values in the "Sentiment" column are determined by the star ratings provided. Ratings higher than 3 have been categorized as "Positive," while the remaining ratings are labeled as "Negative."</p>
            <br><img src="images/b2b_classification_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the Decision Tree algorithm, which exclusively accepts data that are partitioned into two disjoint sets: training and test sets. Hence, the dataset has been split at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/b2b_classification_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/1SK_nSuh8YEwTltD1Uq_kFClbBOQdk14O?usp=drive_link" target="_blank">here</a></p>
            <br><p>Find the data preparation code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Data_Preparation.ipynb" target="_blank">here</a></p>

            <h3>Experiment Setup & Results</h3><br>
            <p>Three Decision Tree Classifiers were built for classification and they yielded the following results:</p>
            <br><img src="images/b2b_DT_result1.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>Decision Tree with entropy criterion and a depth of 100 reduced overfitting and yielded the best result.</p>
            <br><img src="images/b2b_DT_result2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Classification.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>While the Decision Tree achieves a high accuracy of 88%, it prioritizes words that are not directly linked to sentiments simply because splitting the tree based on those words yields high information gain (for example, 'customer'). Consequently, although the model accurately predicts labels, there's a significant chance it may not accurately predict sentiment for complex reviews.</p>
            <br>
        </div>
    </div>

    <div class="tab-contents" id="SVM-Content">
        <div class="intro">
            <h1 class="sub-title">Support Vector Machine Classification</h1>
            <br><br>
            <p>Support Vector Machine (SVM) is a supervised learning algorithm used for classification tasks. In SVM, all data, including categorical data, is represented as quantitative vectors, with each row serving as a vector. It is important to convert the data into quantitative vectors for SVM's reliance on numerical features for the computation optimal decision boundary.</p>
            <br>
            <p>The vectors located on the boundary of the separable margins are termed support vectors. Each SVM model is designed to classify between two categories, such as 0 vs 1. SVM is particularly useful for linearly separable data, where it can effectively create a boundary between classes. However, in cases where data is not linearly separable, SVM employs kernel functions to add extra dimensions to the data, making it linearly separable.</p>
            <br><img src="images/b2b_svm_intro2.PNG" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <br>
            <br><p>Having analyzed the LDA results, we've identified the key themes discussed in each review. Now, our focus shifts to understanding the underlying sentiment or emotion associated with these themes. This crucial step requires the development of a classification model capable of accurately discerning the sentiment expressed in each review.</p>
            <br><p>To achieve this, we've leveraged customers' star ratings in conjunction with their corresponding reviews as the foundational elements for our classification task.SVM is especially powerful for high-dimensional data, making it well-suited for text data. <b>Hence, SVM is used in this section to construct a classification model that effectively predicts the sentiment conveyed within each review, distinguishing between positive and negative sentiments.</b></p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>In order to train SVM algorithms, labeled data is essential. Hence, a new label column called "Sentiment" has been generated. The values in the "Sentiment" column are determined by the star ratings provided. Ratings higher than 3 have been categorized as "Positive," while the remaining ratings are labeled as "Negative."</p>
            <br><img src="images/b2b_classification_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the SVM algorithm, which exclusively accepts data that are partitioned into two disjoint sets: training and test sets. Hence, the dataset has been split at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/b2b_classification_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/1SK_nSuh8YEwTltD1Uq_kFClbBOQdk14O?usp=drive_link" target="_blank">here</a></p>
            <br><p>Find the data preparation code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Data_Preparation.ipynb" target="_blank">here</a></p>

            <h3>Experiment Setup & Results</h3><br>
            <p>Three combinations of SVM kernel and cost parameter were used for classification and they yielded the following results:</p>
            <br><img src="images/b2b_svm_results1.PNG" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>All three combinations yeilded moreover the same result. The top positive and negative coefficients is displayed below. This visualization aids in understanding the importance of each feature (word) in the SVM model's decision-making process.</p>
            <br><img src="images/b2b_svm_results2.png" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Classification.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>In comparison to the Naive Bayes and Decision Tree algorithms, the Support Vector Machine has demonstrated superior performance in accurately classifying sentiments. As a result, this SVM model stands out as a robust tool for detecting the sentiment of future reviews with precision.</p>
            <br>
        </div>
    </div>


    <div class="tab-contents" id="NN-Content">
        <div class="intro">
            <h1 class="sub-title">Neural Netowrk</h1>
            <br><br>
            <p>Neural networks are computer systems inspired by the human brain. Like the neurons that fire and connect, artificial neural networks use interconnected nodes to process information. This allows them to learn and adapt to complex data, making them a powerful tool in machine learning. While traditional machine learning algorithms rely on hand-crafted features, neural networks can discover these features themselves. Additionally, there are many different neural network architectures, each suited for specific tasks. For this project, I have utilized a simple Artificial Neural Network (ANN) to classify sentiments.</p>
            <br>
            <br><img src="images/B2C_NN_Intro.jpg" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <br>
            <br><p>Having analyzed the LDA results, we've identified the key themes discussed in each review. Now, our focus shifts to understanding the underlying sentiment or emotion associated with these themes. This crucial step requires the development of a classification model capable of accurately discerning the sentiment expressed in each review.</p>
            <br><p>To achieve this, we've leveraged customers' star ratings in conjunction with their corresponding reviews as the foundational elements for our classification task.SVM is especially powerful for high-dimensional data, making it well-suited for text data. <b>Hence, Neural Network is used in this section to construct a classification model that effectively predicts the sentiment conveyed within each review, distinguishing between positive and negative sentiments.</b></p>
            <br>

            <h3>Data Preparation</h3><br>
            <p>In order to train the NN algorithms, labeled data is essential. Hence, a new label column called "Sentiment" has been generated. The values in the "Sentiment" column are determined by the star ratings provided. Ratings higher than 3 have been categorized as "Positive," while the remaining ratings are labeled as "Negative."</p>
            <br><img src="images/b2b_classification_dataprep1.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <p>Python is utilized to build the NN algorithm, which exclusively accepts data that are partitioned into two disjoint sets: training and test sets. Hence, the dataset has been split at a ratio of 77% to 33%. They need to be disjoint to avoid information from the test set unintentionally influences the training process, which may lead to overfitting and inflates the performance metric.</p>
            <br><img src="images/b2b_classification_dataprep2.PNG" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><p>Finally, the taget variable was converted to numerical values since NN expects a numerical response variable.</p>
            <br><img src="images/B2C_NN_dataprep.png" class= "DataPrep-Step" style="max-width: 30%;"><br>
            <br><p>Download the data <a href="https://drive.google.com/drive/folders/1SK_nSuh8YEwTltD1Uq_kFClbBOQdk14O?usp=drive_link" target="_blank">here</a></p>
            <br><p>Find the data preparation code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Data_Preparation.ipynb" target="_blank">here</a></p>
            

            <h3>Experiment Setup & Results</h3><br>
            <p>A simple Neural Netowrk with three layers were built, where the first 2 layers were built using relu actiation function and the last layer with sigmoid function. The model was trained for 100 epochs, however the model converged at around 10 epochs:</p>
            <br><img src="images/B2C_NN_model accuracy.png" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <p>The neural network algorithm was tested with a test data and it yeiled an accuracy of  91%.</p>
            <br><img src="images/B2C_NN_model accuracy2.png" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><p>Find the code <a href="https://github.com/Niranjan-Cholendiran/B2C-Brand-Perception-Analysis/blob/main/05_Classification/Classification_Using_NN.ipynb" target="_blank">here</a></p>

            <h3>Conclusion</h3><br>
            <p>Neural Network has yeilded a strong 91% in classifying the sentiments. From observation, it was found out that Uber has been praised by its customers for its Quality and Services with major sentiments as positive, however there were a lot of concerns around Uber driver's behavior with mostly negative reviews.</p>
            <br>
        </div>
    </div>


    <div class="tab-contents" id="Conclusion-Content">
        <div class="intro">
            <h1 class="sub-title">Conclusion</h1>
            <br><br>
            <p>This project developed a framework for monitoring a company's brand perception. This framework was applied to analyze Uber, a leading transportation service through the public opinions shared on Trustpilot. The framework allows companies to track customer sentiment and tailor strategies to improve their market standing.</p>
            <br>
            
            <h3>Global Reach and Local Voice</h3><br>
            <br><p>Uber operates in over 100 countries. Customers from various regions, including the United States, Canada, the United Kingdom, India, and Australia, shared their experiences on Trustpilot. The analysis focused on four key areas of customer feedback:</p>
            <br><img src="images/b2c_conclusion1.png" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;">Convenience and Service: Customers used words like "booking," "app," and "pickup" to describe how easy it is to hail a ride through the Uber app.</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Quality and Comfort: Terms like "clean," "smooth," and "dirty" reflected customer experiences with the vehicles themselves.</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Driver Behavior: Feedback included words like "driver," "friendly," "respectful," and "professional" regarding driver interactions.</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Ride Experience: Customers used words like "bumpy," "delay," "fast," and "convenient" to describe their overall journey.</li>
            </ul>
            <br>
            <h3>Mixed Bag of Reviews</h3><br>
            <br><p>The analysis revealed a mix of positive and negative sentiments across all categories. Customers shared both compliments and complaints.</b></p>
            <br><img src="images/b2c_conclusion2.png" class= "DataPrep-Step" style="max-width: 70%;"><br>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;">Convenience and Service, Ride Experience: These categories received a balanced mix of positive and negative reviews</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Quality and Comfort: This category received the most positive feedback, suggesting that cleanliness and comfort are strengths for Uber.</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Driver Behavior: This category had the most negative feedback, indicating a need for improvement in driver courtesy and professionalism.</li>
            </ul>

            <br>
            <h3>Trends and Insights</h3><br>
            <br><p>The data showed a positive trend in customer perception of quality and comfort over the past three years. This suggests that Uber is delivering well in these areas. However, driver behavior reviews took a dip in late 2022. While there's a recent upward trend in positive reviews, it's important to address this concern.</p>
            <br><img src="images/b2c_conclusion3.png" class= "DataPrep-Step" style="max-width: 50%;"><br>
            <br><img src="images/b2c_conclusion4.png" class= "DataPrep-Step" style="max-width: 50%;"><br>


            <h3>Geographic Nuances</h3><br>
            <br><p>The analysis also revealed geographic variations in sentiment. Customers in India expressed more positive sentiment, likely due to their perception of good ride quality. In contrast, Australian customers leaned towards negative feedback, possibly linked to driver behaviour issues.</p>
            <br><img src="images/b2c_conclusion5.png" class= "DataPrep-Step" style="max-width: 50%;"><br>
            
            <h3>Recommendations for Improvement</h3><br>
            <br><ul>
                <li style="list-style-type: circle; color: #c0c0c0;">Investigate Driver Concerns: A thorough investigation into the reasons behind negative driver behavior reviews in Australia and similar markets is recommended.</li>
                <li style="list-style-type: circle; color: #c0c0c0;">Take Action: Based on the investigation, Uber should implement targeted solutions, such as driver training programs or stricter monitoring mechanisms, to address these concerns and improve customer experience.</li>
            </ul>
            <br>
            <h3>Further Exploration</h3><br>
            <br><p>For a more in-depth analysis, an interactive dashboard can be explored <a href="https://public.tableau.com/views/UberBrandPerceptionAnalysis/UberBrandPerception?:language=en-GB&:sid=&:display_count=n&:origin=viz_share_link" target="_blank">here</a></p>
            
        </div>
    </div>

</div>

<div class="copyright" id="copyright">
</div>

<script>

    var tablinks = document.getElementsByClassName("tab-links");
    var tabcontents = document.getElementsByClassName("tab-contents");

    function opentab(tabname){
        for(tablink of tablinks){
            tablink.classList.remove("active-link");
        }
        for(tabcontent of tabcontents){
            tabcontent.classList.remove("active-tab");
        }
        event.currentTarget.classList.add("active-link");
        document.getElementById(tabname).classList.add("active-tab");
    }

</script>


</body>
</html>